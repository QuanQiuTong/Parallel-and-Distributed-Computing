# 并行分布式计算实验报告：基于MapReduce的WordCount算法实现与性能评估

## 1. 实验背景与目的

MapReduce 作为一种经典的分布式计算编程模型，通过“分而治之”的思想解决了大规模数据集的并行处理问题。本次实验旨在脱离 Hadoop 官方示例库，自主实现 WordCount 算法的 Map 和 Reduce 逻辑。实验的核心目标不仅在于代码实现，更在于通过在不同数据规模（从 1KB 到 1GB）和不同硬件环境下的压力测试，深入探究计算节点的并行度对执行效率的影响，分析加速比（Speedup）的变化规律，并识别单机模拟环境下的系统瓶颈。

## 2. 实验环境与配置

为了对比硬件性能对并行计算的影响，本次实验在两套差异较大的硬件环境中进行。需要特别说明的是，实验采用了 Hadoop 的 `LocalRunner` 模式，即在单个 JVM 进程中通过多线程模拟 MapReduce 的分布式执行。

**环境 A（高性能组）：**
采用 Intel Core i9-12900H 处理器（14核20线程）配合 48GB DDR5 高频内存（4800 MT/s）。在该环境下，`mapred-env` 的堆内存（HEAPSIZE）被放宽至 10GB，以测试高并发下的性能极限。

**环境 B（主流配置组）：**
采用 Intel Core i5-10210U 处理器（4核8线程）配合 16GB DDR4 双通道内存。堆内存限制为 5GB。此环境代表了受限资源下的并行计算表现。

## 3. 算法设计与实现机制分析

### 3.1 Map与Reduce的逻辑实现

本次实验未调用 `hadoop-mapreduce-examples` 库，而是重写了 Mapper 和 Reducer 类。
在 `WordCountMapper` 中，采用了 `StringTokenizer` 对文本行进行切分。虽然相比于正则匹配，`StringTokenizer` 在处理极其复杂的文本清洗时略显不足，但在 WordCount 这种 I/O 密集型任务中，其对象创建开销较小，处理速度较快。Mapper 输出键值对 `<Text, IntWritable>`，其中 Value 固定为 1。

`WordCountReducer` 接收来自 Mapper 阶段输出并经过 Shuffle 分组的键值对 `<Text, Iterable<IntWritable>>`，通过遍历迭代器累加词频，最终输出统计结果。

### 3.2 针对单机环境的切片优化策略

在 Driver 端的代码设计中，有一个至关重要的配置细节：

```java
conf.setLong("mapreduce.input.fileinputformat.split.maxsize", 16 * 1024 * 1024);

```

在 Hadoop 的默认机制中，文件切片（InputSplit）的大小通常由 Block Size（默认 128MB）决定。在 Local 模式下处理 100MB 或 1GB 的文件时，如果不加干预，框架可能会将其视为一个或极少量的切片，导致仅有 1 个 Map 任务启动，无论我们设置了多少 `map.tasks.maximum`。

通过强制将最大切片限制为 16MB，我们人为地将 1GB 的输入数据切分为约 60-64 个切片。这强制 Hadoop 启动多个 Map 任务（线程）来并行处理这些切片，从而确保实验能够真实反映出多线程并行计算的效果，而非仅仅测试单线程的文件读取速度。

## 4. 实验结果与数据分析

（注：本节主要对 1000MB 大数据集的表现进行深入剖析，完整测试数据见附录）

### 4.1 数据规模对加速比的影响（Amdahl定律的体现）

观察附录中的数据可以发现，在 1KB 至 10MB 的小数据集场景下，无论增加多少 Map 或 Reduce 任务，作业的完成时间始终稳定在 1.2秒至 2.4秒之间，甚至在部分高并发设置下有所回升。

这生动地体现了分布式计算系统的**启动开销（Overhead）**。Hadoop 作业的生命周期包括作业上下文的初始化、JobClient 的提交、分片的计算以及 Mapper/Reducer 对象的反射创建等。对于小文件，实际的单词统计计算时间是微秒级的，而框架的启动时间占据了 99% 以上。此时，增加并行度不仅不能加速，反而因为线程上下文切换（Context Switch）和资源竞争增加了额外耗时。

### 4.2 大数据量下的并行加速与资源瓶颈

当数据量达到 1GB（1000MB）时，计算密集度显著提升，并行优势开始显现。以下是对环境 A（i9 + DDR5）中 1GB 数据处理的深度分析：

* **线性加速的局限性**：
在 1 Map 任务时，耗时约 78.9 秒；增加到 4 Map 任务时，耗时降至 28.5 秒。此时的**加速比约为 2.76**。虽然性能显著提升，但并未达到理论上的 4 倍加速。这主要是因为 WordCount 是典型的 **I/O 密集型与内存密集型** 任务，而非纯 CPU 密集型任务。
在 Local 模式下，尽管有 4 个线程在 CPU 上并行计算，但它们共享同一块物理磁盘的 I/O 带宽和同一条内存总线。多线程同时读取磁盘会导致随机 I/O 增加，抵消了部分 CPU 并行的优势。
* **DDR5 与 DDR4 的性能鸿沟**：
对比两套环境的最优成绩（均为 4 Map 4 Reduce）：环境 A 耗时 28.5 秒，环境 B 耗时 58.2 秒。环境 A 的速度几乎是环境 B 的两倍。除了 CPU 核心数的差异外，**内存带宽**起到了决定性作用。WordCount 过程中产生海量的 String 对象和临时 IntWritable 对象，频繁的内存读写和垃圾回收（GC）对内存带宽极其敏感。DDR5 高频内存显著缓解了多线程下的内存争抢问题。

### 4.3 任务失败原因剖析（OOM与资源竞争）

实验中一个引人注目的现象是：在 8 Map 和 16 Map 的高并发设置下，任务频繁失败（Failed）。

在 Hadoop 的 MapReduce 过程中，每个 Map 任务都需要维护一个环形内存缓冲区（由 `mapreduce.task.io.sort.mb` 控制，代码中设为 50MB）用于在内存中对输出结果进行预排序（Spill & Sort）。

* **在环境 B（16GB 内存）中**：开启 8 个 Map 任务意味着仅缓冲区就需要 400MB 内存，再加上 JVM 自身的对象开销、Reduce 任务的内存占用，瞬间的堆内存需求超过了设置的 5GB 限制，直接导致 `java.lang.OutOfMemoryError: Java heap space`。
* **在环境 A（48GB 内存）中**：虽然堆内存放宽到了 10GB，但在 16 Map 任务并发时依然失败。这往往不仅仅是内存总量的问题，更是 **GC（垃圾回收）抖动** 的结果。16 个高频创建对象的线程同时运行，会导致 JVM 的 Eden 区瞬间填满，触发频繁的 Full GC，导致“Stop-the-world”时间过长，Hadoop 的心跳检测机制可能因此判定任务超时或丢失，从而导致 Job 失败。

这证明了在单机模拟分布式计算时，**并行度并非越高越好**，必须在 CPU 核心数、物理内存容量和磁盘 I/O 能力之间寻找平衡点（Sweet Spot）。在本实验中，**4 Map 任务**即为该单机环境下的最佳并行度。

## 5. 结论

通过本次实验，我们得出以下结论：

1. **并行计算的阈值**：MapReduce 带来的并行优势仅在处理中大规模数据集（>100MB）时才能抵消框架自身的启动开销。
2. **资源瓶颈的转移**：随着 CPU 并行度的增加，系统的瓶颈会迅速从 CPU 计算能力转移到内存带宽和磁盘 I/O 上。在单机环境下，内存溢出是限制高并发的主要因素。
3. **Local 模式的参考价值**：虽然 LocalRunner 无法模拟真实集群的网络传输（Shuffle 阶段的网络 I/O），但它能有效验证 Map/Reduce 逻辑的正确性，并能准确反映出多线程环境下的内存压力和 I/O 竞争情况。

---

## 附录：实验原始数据汇总

### 表1：环境 A (i9-12900H, 48GB RAM) 运行时间记录 (单位: ms)

*注：由于 Reducer 数量对 Map 阶段性能影响较小，且日志显示各 Reducer 组别趋势一致，此处主要展示 4 Reducer 组的完整数据以体现最佳性能。*

| 数据规模 | 1 Map Task | 2 Map Tasks | 4 Map Tasks | 8 Map Tasks | 16 Map Tasks |
| --- | --- | --- | --- | --- | --- |
| **1 K** | 1379 | 1356 | 1335 | 1338 | 1350 |
| **1 M** | 1348 | 1416 | 1323 | 1429 | 1401 |
| **10 M** | 2451 | 2476 | 2394 | 2360 | 2464 |
| **100 M** | 7226 | 4267 | 4227 | 4396 | 4462 |
| **1000 M** | 78912 | 42687 | **28566** | 33600 | **Failed** |

### 表2：环境 B (i5-10210U, 16GB RAM) 运行时间记录 (单位: ms)

| 数据规模 | 1 Map Task | 2 Map Tasks | 4 Map Tasks | 8 Map Tasks | 16 Map Tasks |
| --- | --- | --- | --- | --- | --- |
| **1 K** | 1452 | 1443 | 1429 | 1458 | 1507 |
| **1 M** | 1493 | 1411 | 1406 | 1433 | 1440 |
| **10 M** | 2393 | 2421 | 2402 | 2409 | 2401 |
| **100 M** | 11515 | 8448 | 6441 | 6440 | 6479 |
| **1000 M** | 115588 | 73134 | **58211** | **Failed** | **Failed** |

### 表3：1000MB 数据集下的加速比对比（以 4 Reducer 为基准）

| 场景 | 串行耗时 (1 Map) | 最佳并行耗时 (4 Map) | 加速比 (Speedup) |
| --- | --- | --- | --- |
| **环境 A (i9)** | 78.91 s | 28.56 s | **2.76** |
| **环境 B (i5)** | 115.58 s | 58.21 s | **1.98** |