# 多线程并行快速排序算法实验报告

## 一、实验目的

本实验旨在通过实现并测试多线程并行快速排序算法，加深对《并行与分布式计算》中**分治算法并行化**、**任务划分**以及**并行加速比与可扩展性**等核心概念的理解。

---

## 二、算法原理

### 2.1 串行快速排序回顾

快速排序（Quick Sort）是一种典型的**分治算法**，其基本思想如下：

1. 从待排序序列中选择一个元素作为基准（pivot）；
2. 对序列进行一次划分（partition），使得：

   * 小于 pivot 的元素位于左侧；
   * 大于等于 pivot 的元素位于右侧；
3. 分别对左右两个子序列递归执行快速排序。

在平均情况下，快速排序的时间复杂度为 (O(n \log n))。

快速排序的递归结构天然适合并行化处理。

---

### 2.2 并行化思路

在快速排序中，**左右子区间在完成一次 partition 之后相互独立**，因此可以并行执行。这种并行模式属于典型的 **递归任务并行（recursive task parallelism）**。

并行快速排序的基本思想是：

* 在完成一次 partition 后，为左右子区间分别创建并行任务；
* 每个任务递归执行快速排序；
* 当子区间规模较小时，停止创建新任务，转而使用串行排序，以避免线程管理和调度开销。

---

## 三、并行实现方法

### 3.1 并行编程模型选择

本实验采用 **OpenMP** 作为并行编程模型，原因如下：

* OpenMP 支持基于共享内存的多线程并行；
* 提供了 `task` 机制，适合表达递归分治算法；
* 相比显式线程管理（如 pthread），编程复杂度较低，适合课程实验。

---

### 3.2 任务划分与 Cutoff 策略

在递归并行快速排序中，如果对所有子问题都创建并行任务，会导致：

* 任务数量呈指数级增长；
* 线程调度与任务管理开销迅速上升；
* 并行性能反而下降。

因此，本实验引入 **Cutoff 阈值**：

* 当子区间规模小于阈值时，不再创建新任务；
* 直接采用串行快速排序完成剩余工作。

该策略在保证并行度的同时，有效降低了运行时开销。

---

### 3.3 并行快速排序核心流程

并行快速排序的执行流程如下：

1. 在主线程中启动 OpenMP 并行区域；
2. 由单个线程发起第一次并行快速排序调用；
3. 在每次递归中：

   * 若区间规模小于阈值，则串行排序；
   * 否则执行 partition，并为左右子区间创建 OpenMP task；
4. 使用 `taskwait` 等待子任务完成。

该实现未调用任何现成的并行排序库接口，所有并行逻辑均由程序显式实现，符合实验要求。

---

## 四、实验环境与设置

### 4.1 硬件与软件环境

* 处理器：Intel i9-12900H（14 核心 20 线程，其中6个性能P核心8个能效E核心）
* 操作系统：Windows 11 25H2
* 编译器：g++ 11
* 编译选项：`-O3 -fopenmp`
* 并行框架：OpenMP

---

### 4.2 测试数据与参数

* 数据类型：`int`
* 数据分布：均匀随机分布
* 测试规模：

  * 1K、5K、10K、100K、1M、10M、100M
* 线程数配置：

  * 1、2、4、8、12、16、20
* Cutoff 阈值：2000

为保证实验公平性，所有测试使用固定随机种子生成数据。

---

## 五、实验结果与性能分析

### 5.1 执行时间测试结果

Data Size | 1T (ms) | 2T (ms) | 4T (ms) | 8T (ms) | 12T (ms) | 16T (ms) | 20T (ms)
--- | ------- | ------- | ------- | ------- | -------- | -------- | --------
1K | 0.046 | 0.126 | 0.179 | 0.279 | 0.297 | 0.237 | 0.359
5K | 0.198 | 0.249 | 0.254 | 0.329 | 0.517 | 0.305 | 0.311
10K | 0.401 | 0.371 | 0.427 | 0.336 | 0.348 | 0.593 | 0.392
100K | 5.732 | 3.847 | 2.978 | 2.425 | 1.773 | 2.023 | 1.883
1M | 62.659 | 40.168 | 22.748 | 14.363 | 12.174 | 11.618 | 10.598
10M | 622.333 | 538.870 | 347.641 | 170.758 | 147.022 | 124.998 | 120.559
100M | 6588.179 | 4180.481 | 2639.262 | 2030.490 | 1476.903 | 1326.939 | 1151.735

**表1：不同数据规模与线程数下的执行时间**

实验结果表明：

* 在小规模数据（1K–10K）下，多线程并行排序无法取得明显加速；
* 随着数据规模增大（≥100K），并行快速排序开始展现明显性能优势；
* 在百万级及以上数据规模下，多线程版本显著优于串行版本。

---

### 5.2 加速比分析

加速比定义为：

$$ S(p) = \frac{T_1}{T_p} $$

其中 $T_1$ 为单线程执行时间，$T_p$ 为使用 $p$ 个线程的执行时间。

为直观展示并行快速排序在大规模数据下的加速效果，下表给出了 **数据规模为 100M** 时，在不同线程数下的加速比。

| 线程数 | 执行时间 (ms) | 加速比  |
| --- | --------- | ---- |
| 1   | 6588.179  | 1.00 |
| 2   | 4180.481  | 1.58 |
| 4   | 2639.262  | 2.50 |
| 8   | 2030.490  | 3.24 |
| 12  | 1476.903  | 4.46 |
| 16  | 1326.939  | 4.96 |
| 20  | 1151.735  | 5.72 |

**表 2：100M 数据规模下的并行加速比**

可以看到：

1. 随着线程数增加，加速比持续提升，但明显低于理想线性加速；
2. 在 8–12 线程区间内，加速比增长最为显著；
3. 超过 16 线程后，加速收益开始趋缓，反映出内存带宽与调度开销的限制。

---

## 六、结果讨论

### 6.1 小规模数据下的性能下降

在小规模数据下，并行版本性能反而劣于串行版本，其原因包括：

* OpenMP task 创建和销毁开销占主导；
* 线程唤醒和调度成本高于排序本身的计算量；
* 并行算法的固定开销无法被摊薄。

该现象符合并行计算中的经典结论：**并行化并非在所有规模下都有效**。

---

### 6.2 并行效率的限制因素

即使在大规模数据下，并行快速排序的加速比仍低于理想线性加速，主要原因包括：

* 分治递归过程中子区间规模不完全均衡；
* 多线程同时访问共享内存，导致缓存一致性和内存带宽竞争；
* Cutoff 策略虽减少了任务数量，但也限制了最大并行度。

---

## 七、结论

通过本实验，可以得出以下结论：

1. 快速排序算法具有良好的并行化潜力，适合采用任务并行模型实现；
2. 使用 OpenMP task 可以较为自然地表达递归分治并行结构；
3. 并行快速排序在数据规模较大时能显著提升性能；
4. 并行性能受限于任务粒度、调度开销以及内存系统特性；
5. 合理设置 Cutoff 阈值对于并行算法性能至关重要。

本实验加深了对并行分治算法性能特征的理解，也验证了并行计算中“规模决定效果”的基本原则。

---

## 附录：接口实现方式对性能的补充说明

### A.1 不同接口实现方式的性能对比

在完成主要实验之外，进一步对三种不同的数据区间接口实现方式进行了对比：

1. `std::vector<int>&`（原始实现方式）
2. `std::span<int>`（C++20 语义化区间表示）
3. 裸指针 + 长度（`int* + size`）

三种实现均采用相同的并行算法结构、相同的 Cutoff 阈值以及相同的编译优化选项。
只是在递归调用时，传递数据区间的方式不同。这也导致了`#pragma omp task` 处的参数捕获与传递方式不同。

以 **100M 数据规模** 为例，其在不同线程数下的执行时间如下所示：

| 接口方式    | 1T (ms) | 2T (ms) | 4T (ms) | 8T (ms) | 12T (ms) | 16T (ms) | 20T (ms)
| ------- | ------- | ------- | -------- | -------- | -------- | -------- | --------
| vector& | 7933.559 | 4932.193 | 3104.912 | 2301.831 | 1882.155 | 1625.978 | 1486.550
| span    | 8561.693 | 5775.370 | 3482.606 | 2553.622 | 1842.779 | 1792.377 | 1797.596
| pointer | 6588.179 | 4180.481 | 2639.262 | 2030.490 | 1476.903 | 1326.939 | 1151.735

### A.2 结果分析

实验结果表明：

* 在单线程或小规模并行度下，三种接口方式的性能差异不明显；
* 在大规模数据与高线程数条件下，裸指针版本具有最优性能；
* `std::span` 版本在高并发递归任务中表现出更高的运行时开销。

该差异并非简单地由单次元素访问成本造成。在`-O3`选项下，OpenMP之外的代码部分均被编译器高度优化，span与裸指针访问会生成一致的汇编代码。

性能差异主要源于OpenMP task 边界处，即`#pragma omp task shared(...)`, 任务创建与调度时的参数捕获与传递开销。
结构化对象（如 `span`）的捕获与传递成本会被累积放大，裸指针与标量参数在任务调度和运行时管理上具有更低的结构性开销。

该对比结果进一步说明：在并行程序中，接口抽象方式的选择不仅影响代码可读性，也可能对整体性能产生影响，尤其是在高并发任务场景下。
